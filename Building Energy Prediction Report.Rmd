---
title: "Building Energy Prediciton Project"
output: pdf_document
date: "`r Sys.Date()`"
---
# 1) Introduction / Overview
## 1.1) Inspiration 
In late 2019, the ASHRAE - Great Energy Predictor III competition was held on the Kaggle platform. The overall objective was to find the most accurate modeling solutions for the building energy use prediction. The details of the competition is here: https://www.kaggle.com/competitions/ashrae-energy-prediction [1]

The competitors was 4,370 participants in 3,614 teams from 94 countries. They submitted 39,403 predictions. The top 5 winning solutions was published; including solution summary, code, and overview video on Github repository: https://github.com/buds-lab/ashrae-great-energy-predictor-3-solution-analysis [2]

## 1.2) Purpose 
The purpose of the capstone project does not to reproduce winner model with new dataset. The reproduction code requires computer resource, therefore it is not suitable for the edX submission and peer review. In this case, the aim of this project is to simply apply one of the most popular model, and the LightGBM was used by all of the top 5 winners. 

## 1.3) Dataset
The dataset for this capstone project is not the dataset provided for Kaggle competition, nevertheless they are similar features and format. There are two type of data in the similar datetime in 2018 and 2019. 

The first is hourly weather data. The specific weather station is selected according to the building site and location. The data can be directly download using python packages called 'meteostat'. For more information please visit: https://meteostat.net/en/ [3].  

The other is electrical metered data of a specific building, it is available here: https://sgrudata.github.io/ [4]. Those two dataset that necessary for the capstone project is gathered and ready to be downloaded from https://github.com/KornkamonTantiwanit/DataScience_Capstone [5].  

# 2) Method / Analysis 
## 2.1) Download Data
The three data files including (1) weather data in 2018 and 2019, (2) energy metered data in 2018 (Jul to Dec), and (3) energy metered data in 2019 (Jan to Dec). To be noted that the energy metered data in 2018 will be a holdout test dataset. Therefore, it will not be touched until the final evaluation at the end.  
  
After all the files have been downloaded, the message will be shown as below.
```{r, echo=FALSE}
suppressPackageStartupMessages({
  library(readr)
  library(knitr)
  library(dplyr)
  library(ggplot2)
  library(lubridate)
  library(hms)
  library(rsample)
  library(lightgbm)
  library(randomForest)
  library(caret)})
```

```{r, echo=FALSE}
options(timeout = 120)

base_url <- "https://github.com/KornkamonTantiwanit/DataScience_Capstone/raw/main/"
files <- c("weather_hourly.zip", "2018Floor2.zip", "2019Floor2.zip")
urls <- paste0(base_url, files)
names(urls) <- files

all_downloads_successful <- TRUE

for (file_name in names(urls)) {
  tryCatch({
    download.file(urls[[file_name]], destfile = file_name, method = "auto") # Download file
    if (grepl("\\.zip$", file_name)) {
      unzip(file_name, exdir = ".") # Unzip into the current directory
      file.remove(file_name)} # Remove the zip file after unzipping
  }, error = function(e) {
    all_downloads_successful <<- FALSE})}

if (all_downloads_successful) cat("Download complete!") else cat("Some downloads failed.")
```

## 2.2) Read CSV and Visualized Data
Two data file was read and visualization, while the one file is untouched as holdout test data. The first file is weather data in 2018 and 2019. The head and plot of hourly data are shown below. 

```{r, echo=FALSE}
# Weather data 
weather_2018_2019 <- read_csv("weather_hourly.csv", show_col_types = FALSE) %>%
                     mutate(time = as.POSIXct(time))
kable(head(weather_2018_2019), format = "markdown", align = "l")

# Data Visualization 
ggplot(weather_2018_2019, aes(x = time)) +
  geom_line(aes(y = rhum, color = "Relative Humidity (RH)"), linewidth = 0.25) +  
  geom_line(aes(y = temp, color = "Temperature (C)"), linewidth = 0.25) +  
  geom_line(aes(y = dwpt, color = "Dew Point (C)"), linewidth = 0.25) +    
  labs(title = "Hourly Weather Data 2018 & 2019",
       x = "Time", y = "Value") +
  scale_color_manual("",
                     breaks = c("Relative Humidity (RH)", "Temperature (C)", "Dew Point (C)"),
                     values = c("Relative Humidity (RH)" = "blue", 
                                "Temperature (C)" = "black", 
                                "Dew Point (C)" = "cyan")) +
  theme_minimal() +
  theme(legend.position = "bottom")
```
  
The second file is electrical metered data that was read and visualized only for training data in 2019 and leave 2018 untouch as holdout test data. The original format has multiple column as follows, moreover it is recorded in one minute data as (Wh).  

```{r, echo=FALSE}
# Energy data in 2019 only and exclude 201 8 for holdout test data 
Energy_2019 <- read_csv("2019Floor2.csv", show_col_types = FALSE) %>%
               mutate(Date = as.POSIXct(Date))
colnames(Energy_2019)
kable(head(select(Energy_2019, 1:8)), format = "markdown", align = "l")
```

The one minute data is then selected for energy-related column in (Wh) and summed up to hourly data in (kWh). The head and visualization is below:   

```{r, echo=FALSE, warning=FALSE}
sum_energy_kWh <- function(data) {
    data %>%
    mutate(Date = floor_date(Date, "hour")) %>%   # Round down to the hour
    group_by(Date) %>%                            # Group by rounded hour
    summarise(across(contains("kW"), sum, na.rm = TRUE)) %>%  # Sum all columns with "kW"
    rowwise() %>%
    mutate(total_Elec_kWh = sum(c_across(contains("kW")), na.rm = TRUE)) %>%
    ungroup() %>%
    select(Date, total_Elec_kWh)     # Select columns
}

# This part of code takes 3 minutes to run.
Y2019_kWh <- sum_energy_kWh(Energy_2019)
kable(head(Y2019_kWh))

# Data Visualization 
ggplot(Y2019_kWh, aes(x = Date, y = total_Elec_kWh)) +
  geom_line(color = "blue") +
  labs(title = "Total Electricity Consumption 2019",
       x = "Date",
       y = "Total Electricity (kWh)") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
```

The zoomed plot shows the pattern of monthly and a selected weekly for electrical consumption in 2019 respectively.   

```{r, fig.width=6, fig.height=4, echo=FALSE}
# Monthly Plot
Y2019_kWh_month<- Y2019_kWh %>% mutate(Month = format(Date, "%m-%b"))

ggplot(Y2019_kWh_month, aes(x = Date, y = total_Elec_kWh)) +
  geom_line(color = "blue") +  
  labs(title = "Monthly Total Electricity Consumption in 2019",
       x = "Date",
       y = "Total Electricity (kWh)") +
  theme(axis.text.x = element_blank()) +  
  facet_wrap(~ Month, ncol = 4, scales = "free_x")

# Weekly Plot 
Y2019_kWh_week <- Y2019_kWh %>%
                  filter(Date >= as.Date("2019-01-14") & Date < as.Date("2019-01-20"))
ggplot(Y2019_kWh_week, aes(x = Date, y = total_Elec_kWh)) +
  geom_line(color = "blue") + 
  labs(title = "Total Electricity Consumption for a specific week in 2019",
       x = "Date",
       y = "Total Electricity (kWh)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## 2.3) Combined Data for Training  
The hourly weather data and hourly electrical usage in 2019 are combined as a training data, as shown below: 

```{r, echo=FALSE}
# Filter the weather data only for the training year 2019
wea_2019 <- weather_2018_2019 %>% filter(format(time, "%Y") == "2019")

# Combine training and target data into single dataframe
combined_data <- left_join(Y2019_kWh, wea_2019, by = c("Date" = "time"))
kable(head(combined_data), format = "markdown", align = "l")
```

Then split training data in to train set and test set according to the code below: 

```{r}
set.seed(123)
data_split <- initial_split(combined_data, prop = 0.8)
train_set <- training(data_split)
test_set <- testing(data_split)
```

## 2.4) Feature Engineering 
The datetime is treated as a numeric value for training feature purposes, as following:    
- Time of day: working hours is 08:30-16:30  
- Weekday: Monday, Tuesday, Wednesday, Thursday, Friday  
- Weekend: Saturday, Sunday  
- Public holiday: 13 national holidays are 1-Jan, 6-Apr, 13-16 Apr, 1-May, 4-May, 28-July, 12-Aug,  
  13-Sep, 5-Dec, 10-Dec, 31-Dec  

```{r,echo=FALSE }
# Function to process Feature engineering (times of day - weekday - weekend - public holiday) 
feature_engineering <- function(data, start_time, end_time, public_holidays) {
  data %>%
    mutate(Day_of_Week = as.numeric(factor(weekdays(Date), 
                                           levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))),
           Is_Weekend = ifelse(Day_of_Week >= 6, 1, 0),  # Weekend indicator
           Is_Public_Holiday = ifelse(format(Date, "%m-%d") %in% public_holidays, 1, 0),  # Public holiday indicator
           Time_of_Day = hms::as_hms(format(Date, "%H:%M:%S")),  
           Is_Working_Time = ifelse(Is_Weekend == 0 &  # Check if it's a weekday
                                      Is_Public_Holiday == 0 &  # Check if it's not a public holiday
                                      Time_of_Day >= start_time & Time_of_Day <= end_time, 1, 0)) %>% # Check if time is within working hours
    mutate(Month = lubridate::month(Date),
           Day = lubridate::day(Date),
           Hour = lubridate::hour(Date)) %>%
    select(-Time_of_Day, -Month, -Day, -Hour)  
}

start_time <- parse_hms("08:30:00")
end_time <- parse_hms("16:30:00")

# National public holidays (month-day format only)
public_holidays <- c("01-01", 
                     "04-06", "04-13", "04-14", "04-15", 
                     "05-01", "05-04", 
                     "07-28",
                     "08-12",
                     "09-13", 
                     "12-05", "12-10", "12-31")

# Applying the function to the train and test data
train_fea <- feature_engineering(train_set, start_time, end_time, public_holidays)
test_fea <- feature_engineering(test_set, start_time, end_time, public_holidays)
```

## 2.5) Model Training and Evaluation 
This project is explore two models. The first is the most popular model among the top 5 winner of the ASHRAE - Great Energy Predictor III competition. The LightGBM, a tree-based model was applied from all winners. Moreover, the Random Forest which is one of the early developed tree-based model is also study. 

Firstly is the LightGBM. The grid search uses for finding the best model's parameters. The best model is that has the lowest RMSE and the highest r-squared on the test set. 

```{r,echo=FALSE }
# Function to train LightGBM model and evaluate 
train_Lightgbm <- function(train, test, 
                           target_col, params, nrounds = 100,  
                           early_stopping_rounds = 10) {
  train_matrix <- train %>%
    select(-Date, -!!sym(target_col)) %>%  # Exclude the target variable and Date column
    as.matrix()
  train_label <- train[[target_col]]  # Extract target variable
  
  test_matrix <- test %>%
    select(-Date, -!!sym(target_col)) %>%  # Exclude the target variable and Date column
    as.matrix()
  test_label <- test[[target_col]]  # Extract target variable
  
  # Create LightGBM datasets
  dtrain_LightGBM <- lgb.Dataset(data = train_matrix, label = train_label)
  dtest_LightGBM <- lgb.Dataset(data = test_matrix, label = test_label, reference = dtrain_LightGBM)
  
  # Train the model    
  model_LightGBM <- lgb.train(
    params = params,
    data = dtrain_LightGBM,
    nrounds = nrounds,               # Number of boosting iterations
    valids = list(test = dtest_LightGBM),  # Validation set for monitoring performance
    early_stopping_rounds = early_stopping_rounds,
    verbose = -1)  # Early stopping
  
  # Predict on the test set
  preds <- predict(model_LightGBM, test_matrix)
  
  # Calculate RMSE and R-squared
  rmse <- sqrt(mean((preds- test_label)^2))
  ss_total <- sum((test_label - mean(test_label))^2)
  ss_residual <- sum((test_label - preds)^2)
  r_squared <- 1 - (ss_residual / ss_total)
  
  # Return results
  return(list(
    model = model_LightGBM,
    rmse = rmse,
    r_squared = r_squared,
    predictions = preds))
}
#####################################################################################################
# Function to tune LightGBM
tune_Lightgbm <- function(train, test, target_col, tune_grid, nrounds = 100, early_stopping_rounds = 10) {
  results <- list()
  best_rmse <- Inf  
  best_model <- NULL 
  
  for (i in seq_len(nrow(tune_grid))) { # Loop through each combination in the tuning grid
    current_params <- list(
      objective = "regression",
      metric = "rmse",
      learning_rate = tune_grid$learning_rate[i],
      num_leaves = tune_grid$num_leaves[i],
      max_depth = tune_grid$max_depth[i])
    
    # Train the model 
    model_results <- train_Lightgbm(
      train = train,
      test = test,
      target_col = target_col,
      params = current_params,
      nrounds = nrounds,
      early_stopping_rounds = early_stopping_rounds)
    
    # Store the result 
    results[[i]] <- list(
      learning_rate = tune_grid$learning_rate[i],
      num_leaves = tune_grid$num_leaves[i],
      max_depth = tune_grid$max_depth[i],
      rmse = model_results$rmse,
      r_squared = model_results$r_squared)
    
    # Store the best model
    if (model_results$rmse < best_rmse) {
      best_rmse <- model_results$rmse
      best_model <- model_results$model  
    }
  }
  
  # Convert results list to dataframe 
  results_df <- do.call(rbind, lapply(results, as.data.frame))
  return(list(
    tuning_results = results_df,
    best_model = best_model)) 
}
```

This is my setup for grid search parameters. The values of minimum RMSE and maximum R2 on test set is shown. These values will be use for model comparison.     

```{r}
# Tuning grid LightGBM
tune_grid <- expand.grid(learning_rate = c(0.01, 0.05, 0.1),  
                         num_leaves = c(20, 30, 40),          
                         max_depth = c(5, 10, 15))
```

```{r, echo=FALSE, collapse=TRUE}
# Call the tuning function
set.seed(456)
tuning_results_Lightgbm <- tune_Lightgbm(train_fea, test_fea, target_col = "total_Elec_kWh", tune_grid = tune_grid)

# Find the best model and its params
best_rmse_LightGBM <- round(tuning_results_Lightgbm$tuning_result$ rmse
                           [which.min(tuning_results_Lightgbm$tuning_results$rmse)],3)
best_r_squared_LightGBM <- round(tuning_results_Lightgbm$tuning_results$r_squared
                                [which.max(tuning_results_Lightgbm$tuning_results$r_squared)], 3)

cat(paste("Best RMSE on test set (LightGBM):", best_rmse_LightGBM))
cat(paste("R-squared on test set (LightGBM):", best_r_squared_LightGBM))
```

The second model is Random Forest (RF). It is true that RF can not handling missing values. Therefore, the missing values is checked for both train set and test set. If missing values are found, it will not be included and omitted. After the missing values in train set and test set are cleaned, the message will print out as follow: 

```{r,echo=FALSE, collapse=TRUE }
# Function for checking missing values
check_missing <- function(data, dataset_name = "Dataset") {
  if (anyNA(data)) {
    cat("Missing values found in", dataset_name, ":\n")
    print(colSums(is.na(data)))} 
  else {
    cat("No missing values in", dataset_name)}
}

train_fea_clean <- na.omit(train_fea)
test_fea_clean <- na.omit(test_fea)

check_missing(train_fea_clean, "train_fea_clean")
check_missing(test_fea_clean, "test_fea_clean")
```

Again, the grid search uses for finding the best model's parameters. Similar to the LightGBM, the best model has the lowest RMSE and the highest r-squared on the test set. 

```{r,echo=FALSE }
# Train Random Forest 
train_rf <- function(train, test, target_col, ntree = 500, mtry = 3) {
  train_matrix <- train %>%
                  select(-Date, -!!sym(target_col))  # Exclude the target variable and Date column
  train_label <- train[[target_col]]  # Extract target variable
  
  test_matrix <- test %>%
                 select(-Date, -!!sym(target_col))  # Exclude the target variable and Date column
  test_label <- test[[target_col]]  # Extract target variable
  
  # Train the  model
  model_rf <- randomForest(
    x = train_matrix, 
    y = train_label, 
    ntree = ntree, 
    mtry = mtry)
  
  # Predict on the test set
  preds <- predict(model_rf, test_matrix)
  
  # Calculate RMSE and R-squared
  rmse<- sqrt(mean((preds - test_label)^2))
  ss_total <- sum((test_label - mean(test_label))^2)
  ss_residual <- sum((test_label - preds)^2)
  r_squared <- 1 - (ss_residual / ss_total)
  
  # Return results
  return(list(
    model = model_rf,
    rmse = rmse,
    r_squared = r_squared,
    prediction = preds))
}

#####################################################################################################
# Function to tune Random Forest 
tune_rf <- function(train, test, target_col, tune_grid) {
  results <- list()   
  for (i in seq_len(nrow(tune_grid))) { # Loop through each combination in the tuning grid
    current_mtry <- tune_grid$mtry[i]
    current_ntree <- tune_grid$ntree[i]
    
    # Train the  model
    model_results <- train_rf(
      train = train,
      test = test,
      target_col = target_col,
      ntree = current_ntree,
      mtry = current_mtry)
    
    # Store the result 
    results[[i]] <- list(
      mtry = current_mtry,
      ntree = current_ntree,
      rmse = model_results$rmse,
      r_squared = model_results$r_squared)
  }
  
  # Convert results list to a dataframe 
  results_df <- do.call(rbind, lapply(results, as.data.frame))
  return(results_df)
}
```

This is my setup for grid search parameters. The values of minimum RMSE and maximum R2 on test set is shown. These values will be use for model comparison.

```{r}
# Tuning grid Random Forest
tune_grid <- expand.grid(mtry = c(2, 3, 4, 5),  
                         ntree = c(100, 200, 500)) 
```

```{r, echo=FALSE, collapse=TRUE}
# Call the tuning function
set.seed(456)
tuning_results_rf <- tune_rf(train_fea_clean, test_fea_clean, 
                             target_col = "total_Elec_kWh", tune_grid = tune_grid)

best_rmse_rf <- round(tuning_results_rf$rmse
                            [which.min(tuning_results_rf$rmse)],3)
best_r_squared_rf <- round(tuning_results_rf$r_squared
                                 [which.max(tuning_results_rf$r_squared)], 3)
cat(paste("Best RMSE on test set (Random Forest):", best_rmse_rf))
cat(paste("R-squared on test set (Random Forest):", best_r_squared_rf))
```

## 2.6) Model Comparison and Selection  
The RMSE and R2 from the best tune models are compared as the following:

```{r, echo=FALSE}
# Model comparison using RMSE and R2 
comparison <- data.frame(
  Model = c("LightGBM", "Random Forest"),
  RMSE = c(best_rmse_LightGBM, best_rmse_rf),
  R_squared = c(best_r_squared_LightGBM, best_r_squared_rf))
kable(comparison, caption = "Comparison of RMSE and R-squared")
```

The RMSE and R2 of the two models almost similar. In this case, the LightGBM is selected due to its ability to handling missing data. This ability would be specially benefits to the real world data, which usually have missing values. 

## 2.7) Understand the best LightGBM Model 
To understand the best model, the feature importance is studied. The Gain value that measures the reduction in the loss function (error) is considered. it is found that the working hours the most importance feature that could increase or decrease the accuracy of the model.   

```{r, echo=FALSE}
# The best LightGBM 
best_Lightgbm_model <- tuning_results_Lightgbm$best_model

# Extract feature importance from the best model
importance <- lgb.importance(best_Lightgbm_model, percentage = TRUE)
kable(importance)
```

In addition, the best model's parameters is extracted as following. This best tune hyperparameters will be used in the process. 

```{r, echo=FALSE}
# Extract params from the best model
best_params <- list(
  learning_rate = tuning_results_Lightgbm$best_model$params$learning_rate,
  num_leaves = tuning_results_Lightgbm$best_model$params$num_leaves,
  max_depth = tuning_results_Lightgbm$best_model$params$max_depth)

best_params_df <- as.data.frame(t(best_params))
kable(best_params_df, caption = "Best LightGBM Parameters")
```

## 2.8) Accuracy Improvement 
According to the findings above that the working hour is the most importance feature reflects the accuracy of the model. Therefore, the working hour is adjusted from 8:30-16:30 to 8:00-16:00. Then retrain the best tuned LightGBM model with the same seed to confirm the consistence of the results. The RMSE and R2 of the best tuned model and the importance feature adjustment are compared as follow:    

```{r, echo=FALSE}
# Adjust importance feature and retrain the model
start_time_imp <- parse_hms("08:00:00")
end_time_imp <- parse_hms("16:00:00")

train_fea_imp <- feature_engineering(train_set, start_time_imp, end_time_imp, public_holidays)
test_fea_imp <- feature_engineering(test_set, start_time_imp, end_time_imp, public_holidays)

set.seed(456)
result_imp <- train_Lightgbm(train_fea_imp, test_fea_imp, "total_Elec_kWh", best_params)

rmse_LightGBM_imp <- round(result_imp$rmse, 3)
r_squared_LightGBM_imp <- round(result_imp$r_squared, 3)

# Compare Best model and Model improvement 
comparison <- data.frame(
  Model = c("Best Tuned Model", "Importance Feature Adjustment"),
  RMSE = c(best_rmse_LightGBM, rmse_LightGBM_imp),
  R_squared = c(best_r_squared_LightGBM, r_squared_LightGBM_imp))
kable(comparison, caption = "Comparison of RMSE and R-squared")
```

It is clearly seen that adjustment on the most importance feature helps to increase the accuracy of the predictions. 

# 3) Results
The final evaluation is on unseen data shown below. The RMSE and R2 is calculated based on holdout test data, the electrical metered data in 2018.   

```{r, echo=FALSE, collapse=TRUE}
# Read csv the holdout test set 
Energy_2018 <- read_csv("2018Floor2.csv", show_col_types = FALSE) %>%
  mutate(Date = as.POSIXct(Date))

# Preparing data from kW to kWh
Y2018_kWh <- sum_energy_kWh(Energy_2018)
wea_2018 <- weather_2018_2019 %>% filter(format(time, "%Y") == "2018")
combined_holdout <- left_join(Y2018_kWh, wea_2018, by = c("Date" = "time"))

# Feature engineering 
holdout_fea <- feature_engineering(combined_holdout, start_time_imp, end_time_imp, public_holidays)

# preparing data for LightGBM
holdout_matrix <- holdout_fea %>%
  select(-Date, -!!sym("total_Elec_kWh")) %>%  # Exclude the target variable and Date column
  as.matrix()
holdout_label <- holdout_fea[["total_Elec_kWh"]] # Extract target variable

holdout_preds <- predict(best_Lightgbm_model,holdout_matrix)

holdout_rmse <- sqrt(mean((holdout_preds - holdout_label)^2))
holdout_ss_total <- sum((holdout_label - mean(holdout_label))^2)
holdout_ss_residual <- sum((holdout_label - holdout_preds)^2)
holdout_r_squared <- 1 - (holdout_ss_residual / holdout_ss_total)

cat("RMSE on holdout test set:", round(holdout_rmse,3))
cat("R-squared on holdout test set:", round(holdout_r_squared,3))
```

# 4) Conclusion
In summary, the project studied two tree-based model, LightGBM and Random Forest. The evaluation based on test set of simple train-test split data was found similar. However, the LightGBM was selected due to its ability to handling the missing values and reflecting the real world problem. The feature importance of the best tuned LightGBM model was explore. Later, the importance feature was adjusted and the accuracy increased. Finally, the adjusted importance feature LightGBM model was used to predict the unseen data. The R2 was found 0.797 which is satisfied, and shows that the model is well generalized to the unseen data.   

# 5) Reference
1) https://www.kaggle.com/c/ashrae-energy-prediction: The ASHRAE - Great Energy Predictor III competition held on Kaggle platform.
2) https://github.com/buds-lab/ashrae-great-energy-predictor-3-solution-analysis: The repository contains the code and documentation of top-5 winning solutions from the ASHRAE - Great Energy Predictor III competition.
3) https://meteostat.net/en/: Weather and climate database providing detailed weather data for thousands of weather stations and places worldwide.
4) https://sgrudata.github.io/: Building-level Electricity Consumption and Environmental Sensor Data.
5) https://github.com/KornkamonTantiwanit/DataScience_Capstone:  Dataset for edX course, Data Science: Capstone.
6) https://chatgpt.com/:  Computer Coding Tutor using GPT-4o    




